
<!-- saved from url=(0065)http://www.statisticalassociates.com/validityandreliability10.htm -->
<html><head><meta http-equiv="Content-Type" content="text/html; charset=windows-1252">
</head><body><title="validity and="" reliability="" 10="" worst="" pitfalls="" mistakes"="">
<meta http-equiv="content-type" name="description" content="validity, reliability, Cohen&#39;s kappa. ICC, Krippendorff&#39;s alpha, statistical assumptions,  pitfalls, mistakes">


<font face="Arial, Helvetica, Sans Serif" size="+2">
<a href="http://www.statisticalassociates.com/">
Statistical Associates Publishers
</a>
<p>
</p><center>
<h2>Validity and Reliability: 10 Worst Pitfalls and Mistakes</h2>
</center>
<p>
<br>
</p><ul>

<ol>
<li> <i> Establishing convergent validity but not discriminant validity.</i>
<br>
<ul>
When validating the scale used to measure a construct, it is not enough to show convergent validity - that the items "belong together". It is also necessary to show separation from other constructs and their indicator variables - to show discriminant (divergent) validity. It is especially important to show discriminant validity of a predictor construct vis-a-vis its dependent construct.
</ul>

<p>
</p></li><li> <i>Not understanding the different types of scales and the implications of each.</i>
<br>
<ul>
While most scales are measured by sets of like items which intercorrelate well, this is only true of similarity scales. Items do not intercorrelate highly for the other two major types of scales and therefore similarity measures like Cronbach's alpha are inappropriate: (1) difficulty scales, where answering a more difficult item implies what is likely to be answered to less difficult items but where the reverse is not true (e.g., social distance scales); and (2) composite scales, where the sum of the indicator variables measures the construct when indicators represent all dimensions (e.g., measuring "philanthropy" by summing religious giving, educational giving, environmental giving, and so on for all dimensions of giving). 
</ul>


<p>
</p></li><li> <i>Thinking Cronbach's alpha or its variants establish the unidimensionality of the construct being measured.  </i>.
<br>
<ul>
Cronbach's alpha does not establish unidimensionality. Factor analytic methods must be used in conjunction with or instead of Cronbach's alpha.
</ul>



<p>
</p></li><li> <i>Not understanding Cronbach's alpha is a lower bound measure of reliability. </i>.
<br>
<ul>
 Cronbach's alpha is a lower bound on reliability. Other reliability coefficients may well more accurately reflect true reliability.
</ul>



<p>
</p></li><li> <i> Violating the data level assumptions of a statistic.  </i>.
<br>
<ul>
 Some coefficients, such as Cronbach's alpha and ICC, assume continuous (interval or ratio) data. Ordinal reliability coefficients should be used with ordinal data. 
</ul>



<p>
</p></li><li> <i> Not understanding that modern approaches to validation call for a multi-criterion approach.  </i>.
<br>
<ul>
Our "Validity and Reliability 2016 Edition" discusses convergent validity, discriminant validity, criterion validity, content validity, internal validity, experimental validity, longitudinal validity, external validity, and statistical validity.
</ul>





<p>
</p></li><li> <i> Not refining the instrument  </i>.
<br>
<ul>
 Some indicator items scale and discriminate better than others. Development of a survey or test instrument should involve a minimum of two iterations. The researcher should start initially with more indicator items than needed to measure a construct. Between iterations weaker scale items are dropped, refining the way the construct is measured.
</ul>




<p>
</p></li><li> <i> Neglecting factor analytic measures of reliability </i>.
<br>
<ul>
Factor-analytic measures of reliability have come into greater use for continuous data and are preferred by many researchers. These include average variance extracted (AVE), composite reliability (Raykov's rho), and the Fornell &amp; Larcker criterion. 
</ul>

<p>
</p></li><li> <i>Relying on the intra-class correlation coefficient for interrater reliability  </i>.
<br>
<ul>
Since 2007, Krippendorff's alpha has eclipsed ICC as the preferred approach to interrater reliability. It comes in versions for nominal, ordinal, and interval data. 
</ul>

<p>
</p></li><li> <i>Meeting the assumptions of reliability analysis. </i>.
<br>
<ul>
 Our book, listed below, enumerates key assumptions in reliability analysis, clearly listed in the "Assumptions" section.
</ul>



<p>

</p></li></ol></ul>

<p>
</p><hr>
</font><p><font face="Arial, Helvetica, Sans Serif" size="+2">
</font>
</p><center>
<h2>Want to learn more about all this and much more?</h2>
<p>


<a href="http://www.amazon.com/dp/B00BKP6BQ6" target="newvalidity">"Validity and Reliability 2016 Editions" on Amazon</a>, Kindle format. Send your receipt to sa.publiushers@gmail.com to request a free no-password pdf copy. </p><p>

<a href="http://www.statisticalassociates.com/validityandreliability_p.pdf" target="newcluster1">"Validity and Reliability" Preview</a>, PDF format </p><p>

<a href="http://www.statisticalassociates.com/validityandreliability.htm" target="new32411">"Validity and Reliability" Information and table of contents</a> </p><p>


<a href="http://www.amazon.com/dp/1626380201" target="new3883">"Statistical Associates Library" of 50 Statistics E-books on Amazon</a>, no-password .PDF format
</p><p>
</p></center>
<hr>
<p>
<br>



<b> </b>
</p><p></p></title="validity></body></html>