
<!-- saved from url=(0051)http://www.statisticalassociates.com/logistic10.htm -->
<html><head><meta http-equiv="Content-Type" content="text/html; charset=windows-1252">
</head><body><title="logistic regression="" 10="" worst="" pitfalls="" and="" mistakes"="">
<meta http-equiv="content-type" name="description" content="logistic regression, binary logistic regression, multinomial logistic regression pitfalls mistakes">


<font face="Arial, Helvetica, Sans Serif" size="+2">
<a href="http://www.statisticalassociates.com/">
Statistical Associates Publishers
</a>
<p>
</p><center>
<h2>Logistic Regression: 10 Worst Pitfalls and Mistakes</h2>
</center>
<p>
<br>
</p><ul>

<ol>
<li> <i>Not having truly binary data for the dependent variable in binary logistic regression.</i>
<br>
<ul>
If you have an underlying normal distribution for your dichotomous variable, as you would for income = 0 = low and income = 1 = high, probit regression is more appropriate.
</ul>

<p>
</p></li><li> <i>Not having unordered categories for the dependent variable in multinomial logistic regression.</i>
<br>
<ul>
If there is an ascending or descending order to your dependent variable, ordinal regression is more appropriate as it has more power than multinomial logistic regression.
</ul>


<p>
</p></li><li> <i>Not having linearity in the logit</i>.
<br>
<ul>
The right-hand predictor side of the equation must be linear with the left-hand outcome side of the equation. You must test for linearity in the logit (in logistic regression the logit is the outcome side). This is commonly done with the Box-Tidwell Transformation (Test): Add to the logistic model interaction terms which are the crossproduct of each independent times its natural logarithm [(X)ln(X)]. If these terms are significant, then there is nonlinearity in the logit. This method is not sensitive to small nonlinearities. 
</ul>



<p>
</p></li><li> <i>Using classification tables to report model strength when the research purpose is causal analysis.</i>.
<br>
<ul>
Classification tables are used when the research purpose is prediction, not causal analysis. This is because classification tables reward only prediction and not near-miss estimates, unlike pseudo R-squared measures.</ul>



<p>
</p></li><li> <i>Using ROC curves to compare models when the research purpose is causal analysis.</i>.
<br>
<ul>
ROC tables are based on classification table results and have the same problem as mentioned above from the point of view of causal rather than predictive analysis.
</ul>



<p>
</p></li><li> <i>In classification tables, comparing percent correct against the wrong baseline. </i>.
<br>
<ul>
 In predictive analysis, the baseline for percent correct is not 1/L, where L is the number of levels of the dependent variable (not necessarily 1/2 = .50 for a binary dependent, for example). The common baseline for percent correct by chance is the proportion that the most numerous category is of the total. Thus a percent correct of 75% does not improve prediction at all if the most numerous category is 75% or more of the total. 
</ul>


<p>
</p></li><li> <i>Reporting pseudo R-squared measures as percent of variance explained in the dependent variable. </i>.
<br>
<ul>
  This is just incorrect. Report such measures in terms of weak, moderate, or strong. Common cutoffs are 0 - .3, .3 - .6, and .6 - 1.0 respectively.
</ul>







<p>
</p></li><li> <i>Lack of sampling adequacy in factor space. </i>.
<br>
<ul>
It is not just that you need to have adequate sample size. You also need to have adequate count in each cell formed by the factors in your analysis. All cell frequencies should be greater than 1 and 80% or more of cells are should be greater than 5 in count. The presence of small or empty cells may cause the logistic model to become unstable, reporting implausibly large b coefficients and odds ratios for independent variables. 
</ul>




<p>
</p></li><li> <i>Using significance tests when you do not have a random sample. </i>.
<br>
<ul>
 If you have a random sample, you can generalize to the population from which it is drawn if a logistic coefficient is significant. If you have an enumeration (all the cases in the population to which you wish to generalize), significance testing is irrelevant. If you have a non-random sample, significance tests will be in error to an unknown degree. No, bootstrapped significance tests will not solve this problem of inability to generalize, though they do help when the the sample is random but the distribution is non-normal or unknown.
</ul>


<p>
</p></li><li> <i>For reporting effect size, relying on the odds ratio alone. </i>.
<br>
<ul>
Odds ratios are aummary measures of effect size. It is necessary to use marginal analysis (discussed in our book) to understand how effect size is conditioned on the range of values of the covariates. 
</ul>


<p>
</p></li><li> <i>Not meeting the assumptions of logistic regression. </i>.
<br>
<ul>
 Our book, listed below, enumerates 16 assumptions of logistic regression, clearly listed in the "Assumptions" section.
</ul>



<p>

</p></li></ol></ul>

<p>
</p><hr>
</font><p><font face="Arial, Helvetica, Sans Serif" size="+2">
</font>
</p><center>
<h2>Want to learn more about all this and much more?</h2>
<p>


<a href="http://www.amazon.com/dp/B007YAO4XI" target="newlogistic">"Logistic Regression" on Amazon</a>, Kindle format </p><p>

<a href="http://www.statisticalassociates.com/logistic_p.pdf" target="newlogistic">"Logistic Regression" Preview</a>, PDF format </p><p>

<a href="http://www.statisticalassociates.com/logistic.htm" target="newlogistic">"Logistic Regression" Information and table of contents</a> </p><p>


<a href="http://www.amazon.com/dp/1626380201" target="new33">"Statistical Associates Library" of 50 Statistics E-books on Amazon</a>, no-password .PDF format
</p><p>
</p></center>
<hr>
<p>
<br>



<b> </b>
</p><p></p></title="logistic></body></html>